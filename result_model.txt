import torch.nn as nn
    
class ConvNet(nn.Module):
    
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv = nn.Sequential(
          # input: 1x216x216
          nn.Conv2d(1, 64, 5),
          # output: 64x212x212
          nn.LeakyReLU(0.2),
          nn.MaxPool2d(2, 2),
          # output: 64x106x106
          nn.Conv2d(64, 64, 5),
          # output: 64x102x102
          nn.LeakyReLU(0.2),
          nn.MaxPool2d(2, 2),
          # output: 64x51x51
          nn.Conv2d(64, 64, 4),
          # output: 64x48x48
          nn.LeakyReLU(0.2),
          nn.MaxPool2d(2, 2),
          # output: 64x24x24
          nn.Conv2d(64, 64, 5),
          # output: 64x20*20
          nn.LeakyReLU(0.2),
          nn.MaxPool2d(2, 2),
          # output: 64x10*10
        )
        self.fc = nn.Sequential(
          nn.Linear(64*10*10,364),
          nn.LeakyReLU(0.2),
          nn.Dropout(0.5),
          nn.Linear(364,192),
          nn.LeakyReLU(0.2),
          nn.Dropout(0.5),
          nn.Linear(192,10)
        )
    
    def forward(self, input):
        output = self.conv(input)
        output = output.view(output.size(0), 64*10*10)
        output = self.fc(output)
        return output
		
20%



import torch.nn as nn
    
class Conv1DNet(nn.Module):
    
    def __init__(self):
        super(Conv1DNet, self).__init__()
        self.conv = nn.Sequential(
          # input: 1x216x216
          nn.Conv2d(1, 64, (5,1)),
          # output: 64x212x216
          nn.BatchNorm2d(64,momentum=0.9),
          nn.ReLU(),
          nn.MaxPool2d((2,1), (2,1)),
          nn.Dropout(0.5),
          # output: 64x106x216
          nn.Conv2d(64, 64, (5,1)),
          # output: 64x102x216
          nn.BatchNorm2d(64,momentum=0.9),
          nn.ReLU(),
          nn.MaxPool2d((2,1), (2,1)),
          nn.Dropout(0.5),
          # output: 64x51x216
          nn.Conv2d(64, 64, (4,1)),
          # output: 64x48x216
          nn.BatchNorm2d(64,momentum=0.9),
          nn.ReLU(),
          nn.MaxPool2d((2,1), (2,1)),
          nn.Dropout(0.5),
          # output: 64x24x216
          nn.Conv2d(64, 64, (5,1)),
          # output: 64x20x216
          nn.BatchNorm2d(64,momentum=0.9),
          nn.ReLU(),
          nn.MaxPool2d((2,1), (2,1)),
          nn.Dropout(0.5)
          # output: 64x10x216
        )
        self.fc = nn.Sequential(
          nn.Linear(64*10*216,364),
          nn.LeakyReLU(0.2),
          nn.Dropout(0.5),
          nn.Linear(364,192),
          nn.LeakyReLU(0.2),
          nn.Dropout(0.5),
          nn.Linear(192,10)
        )
    
    def forward(self, input):
        output = self.conv(input)
        output = output.view(output.size(0), 64*10*216)
        output = self.fc(output)
        return output
		
26%



import torch.nn as nn
    
class Conv1DNet2(nn.Module):
    
    def __init__(self):
        super(Conv1DNet2, self).__init__()
        self.conv = nn.Sequential(
          # input: 1x216x216
          nn.Conv2d(1, 128, (5,1)),
          # output: 128x212x216
          nn.BatchNorm2d(128,momentum=0.9),
          nn.LeakyReLU(0.2),
          nn.MaxPool2d(2),
          nn.Dropout(0.5),
          # output: 128x106x108
          nn.Conv2d(128, 64, (5,1)),
          # output: 64x102x108
          nn.BatchNorm2d(64,momentum=0.9),
          nn.LeakyReLU(0.2),
          nn.MaxPool2d(2),
          nn.Dropout(0.5),
          # output: 64x51x54
          nn.Conv2d(64, 64, (4,1)),
          # output: 64x48x54
          nn.BatchNorm2d(64,momentum=0.9),
          nn.LeakyReLU(0.2),
          nn.MaxPool2d(2),
          nn.Dropout(0.5),
          # output: 64x24x27
          nn.Conv2d(64, 64, (5,1)),
          # output: 64x20x27
          nn.BatchNorm2d(64,momentum=0.9),
          nn.LeakyReLU(0.2),
          nn.MaxPool2d((2,1), (2,1)),
          nn.Dropout(0.5)
          # output: 64x10x27
        )
        self.fc = nn.Sequential(
          nn.Linear(64*10*27,364),
          nn.LeakyReLU(0.2),
          nn.Dropout(0.5),
          nn.Linear(364,192),
          nn.LeakyReLU(0.2),
          nn.Dropout(0.5),
          nn.Linear(192,10)
        )
    
    def forward(self, input):
        output = self.conv(input)
        output = output.view(output.size(0), 64*10*27)
        output = self.fc(output)
        return output
		
32%


step 20000
size 100000

Epoch 1/50: train_loss: 4.8037, train_accuracy: 94.1086, val_loss: 35.1491, val_accuracy: 10.0406
Epoch 2/50: train_loss: 1.2267, train_accuracy: 90.6104, val_loss: 36.4651, val_accuracy: 10.0406
Epoch 3/50: train_loss: 1.0968, train_accuracy: 91.7568, val_loss: 102.5545, val_accuracy: 10.0406
Epoch 4/50: train_loss: 1.1078, train_accuracy: 90.9945, val_loss: 25.9348, val_accuracy: 10.0406
Epoch 5/50: train_loss: 0.8443, train_accuracy: 91.5441, val_loss: 55.5147, val_accuracy: 10.0406
Epoch 6/50: train_loss: 1.1281, train_accuracy: 93.0272, val_loss: 130.4482, val_accuracy: 10.0406
Epoch 7/50: train_loss: 1.1751, train_accuracy: 93.8072, val_loss: 88.5883, val_accuracy: 10.0406
Epoch 8/50: train_loss: 1.3486, train_accuracy: 93.5768, val_loss: 48.8320, val_accuracy: 10.0406
Epoch 9/50: train_loss: 1.1276, train_accuracy: 90.6518, val_loss: 18.4086, val_accuracy: 10.0406
Epoch 10/50: train_loss: 0.8938, train_accuracy: 91.8572, val_loss: 19.3400, val_accuracy: 10.0406
Epoch 11/50: train_loss: 0.6395, train_accuracy: 92.6786, val_loss: 73.8356, val_accuracy: 9.9053
Epoch 12/50: train_loss: 0.7100, train_accuracy: 92.5250, val_loss: 18.6669, val_accuracy: 10.0406
Epoch 13/50: train_loss: 0.7356, train_accuracy: 93.5000, val_loss: 30.2811, val_accuracy: 10.0406
Epoch 14/50: train_loss: 0.6591, train_accuracy: 92.6372, val_loss: 45.5229, val_accuracy: 10.0406
Epoch 15/50: train_loss: 0.8974, train_accuracy: 93.8722, val_loss: 58.3974, val_accuracy: 11.9337
Epoch 16/50: train_loss: 0.8852, train_accuracy: 93.4291, val_loss: 152.3970, val_accuracy: 10.0744
Epoch 17/50: train_loss: 0.8865, train_accuracy: 92.6313, val_loss: 54.2194, val_accuracy: 10.6153
Epoch 18/50: train_loss: 1.0319, train_accuracy: 93.1868, val_loss: 33.4335, val_accuracy: 10.0406
Epoch 19/50: train_loss: 1.1769, train_accuracy: 93.6595, val_loss: 102.6149, val_accuracy: 10.0406
Epoch 20/50: train_loss: 1.1041, train_accuracy: 92.6904, val_loss: 85.3900, val_accuracy: 10.0406
Epoch 21/50: train_loss: 0.8798, train_accuracy: 94.3627, val_loss: 54.6938, val_accuracy: 10.9533
Epoch 22/50: train_loss: 0.9989, train_accuracy: 93.4113, val_loss: 44.3094, val_accuracy: 12.2380
Epoch 23/50: train_loss: 0.9131, train_accuracy: 93.0982, val_loss: 87.8448, val_accuracy: 12.1028



stepsize 70000
windows size 100000

Epoch 1/50: train_loss: 2.2510, train_accuracy: 21.7930, val_loss: 1.9625, val_accuracy: 26.8571
Epoch 2/50: train_loss: 1.8880, train_accuracy: 30.6390, val_loss: 1.6134, val_accuracy: 43.1429
Epoch 3/50: train_loss: 1.6135, train_accuracy: 42.0124, val_loss: 1.4725, val_accuracy: 48.7143
Epoch 4/50: train_loss: 1.3978, train_accuracy: 50.0000, val_loss: 1.3927, val_accuracy: 49.2857
Epoch 5/50: train_loss: 1.2733, train_accuracy: 54.8641, val_loss: 1.4379, val_accuracy: 51.1429
Epoch 6/50: train_loss: 1.1439, train_accuracy: 60.4196, val_loss: 1.3010, val_accuracy: 58.5714
Epoch 7/50: train_loss: 1.0627, train_accuracy: 62.2794, val_loss: 1.1490, val_accuracy: 61.4286
Epoch 8/50: train_loss: 0.9785, train_accuracy: 65.7606, val_loss: 1.1592, val_accuracy: 63.8571
Epoch 9/50: train_loss: 0.9011, train_accuracy: 68.6695, val_loss: 1.1605, val_accuracy: 63.2857
Epoch 10/50: train_loss: 0.8661, train_accuracy: 70.0048, val_loss: 1.0466, val_accuracy: 67.7143
Epoch 11/50: train_loss: 0.7889, train_accuracy: 72.5322, val_loss: 1.0575, val_accuracy: 68.7143
Epoch 12/50: train_loss: 0.7416, train_accuracy: 74.4635, val_loss: 1.1619, val_accuracy: 63.7143
Epoch 13/50: train_loss: 0.7067, train_accuracy: 75.5365, val_loss: 1.0483, val_accuracy: 68.7143
Epoch 14/50: train_loss: 0.6296, train_accuracy: 78.7554, val_loss: 1.0185, val_accuracy: 71.1429
Epoch 15/50: train_loss: 0.5988, train_accuracy: 79.6137, val_loss: 0.9628, val_accuracy: 71.1429
Epoch 16/50: train_loss: 0.5436, train_accuracy: 81.3543, val_loss: 0.9943, val_accuracy: 72.2857
Epoch 17/50: train_loss: 0.5269, train_accuracy: 81.8312, val_loss: 1.1200, val_accuracy: 70.7143
Epoch 18/50: train_loss: 0.4969, train_accuracy: 83.2856, val_loss: 1.2251, val_accuracy: 67.1429
Epoch 19/50: train_loss: 0.4465, train_accuracy: 84.8832, val_loss: 1.6386, val_accuracy: 62.8571
Epoch 20/50: train_loss: 0.4696, train_accuracy: 83.4287, val_loss: 0.9844, val_accuracy: 73.0000
Epoch 21/50: train_loss: 0.4200, train_accuracy: 85.6700, val_loss: 1.0233, val_accuracy: 72.5714
Epoch 22/50: train_loss: 0.4120, train_accuracy: 86.5522, val_loss: 1.0756, val_accuracy: 71.2857
Epoch 23/50: train_loss: 0.3564, train_accuracy: 87.9113, val_loss: 1.0450, val_accuracy: 72.0000
Epoch 24/50: train_loss: 0.3800, train_accuracy: 87.6729, val_loss: 1.2913, val_accuracy: 68.5714
Epoch 25/50: train_loss: 0.3416, train_accuracy: 88.2928, val_loss: 1.1457, val_accuracy: 73.0000
Epoch 26/50: train_loss: 0.3659, train_accuracy: 87.9828, val_loss: 1.0944, val_accuracy: 70.7143
Epoch 27/50: train_loss: 0.3295, train_accuracy: 89.2942, val_loss: 1.3911, val_accuracy: 69.8571
Epoch 28/50: train_loss: 0.3214, train_accuracy: 89.8188, val_loss: 1.1164, val_accuracy: 73.7143
Epoch 29/50: train_loss: 0.2881, train_accuracy: 90.6056, val_loss: 1.2672, val_accuracy: 73.4286
Epoch 30/50: train_loss: 0.2595, train_accuracy: 90.7725, val_loss: 1.4667, val_accuracy: 72.4286
Epoch 31/50: train_loss: 0.3088, train_accuracy: 89.5565, val_loss: 1.0746, val_accuracy: 74.7143
Epoch 32/50: train_loss: 0.2756, train_accuracy: 91.1063, val_loss: 1.1490, val_accuracy: 75.0000
Epoch 33/50: train_loss: 0.2463, train_accuracy: 91.9170, val_loss: 1.3384, val_accuracy: 73.2857
Epoch 34/50: train_loss: 0.2559, train_accuracy: 91.7501, val_loss: 1.1244, val_accuracy: 75.4286
Epoch 35/50: train_loss: 0.2443, train_accuracy: 92.0362, val_loss: 1.2859, val_accuracy: 73.2857
Epoch 36/50: train_loss: 0.3044, train_accuracy: 90.6533, val_loss: 0.9667, val_accuracy: 75.7143
Epoch 37/50: train_loss: 0.2212, train_accuracy: 93.3238, val_loss: 1.1637, val_accuracy: 74.8571
Epoch 38/50: train_loss: 0.2515, train_accuracy: 92.2508, val_loss: 1.2553, val_accuracy: 70.5714
Epoch 39/50: train_loss: 0.2688, train_accuracy: 91.4878, val_loss: 1.0036, val_accuracy: 75.0000
Epoch 40/50: train_loss: 0.2327, train_accuracy: 92.6562, val_loss: 1.0907, val_accuracy: 75.1429
Epoch 41/50: train_loss: 0.2311, train_accuracy: 92.6800, val_loss: 1.0386, val_accuracy: 75.8571
Epoch 42/50: train_loss: 0.2442, train_accuracy: 92.2985, val_loss: 1.1572, val_accuracy: 75.4286
Epoch 43/50: train_loss: 0.1796, train_accuracy: 93.7768, val_loss: 1.6658, val_accuracy: 74.0000
Epoch 44/50: train_loss: 0.2013, train_accuracy: 94.0153, val_loss: 1.3702, val_accuracy: 73.5714
Epoch 45/50: train_loss: 0.2461, train_accuracy: 92.4416, val_loss: 1.0186, val_accuracy: 74.4286
Epoch 46/50: train_loss: 0.2151, train_accuracy: 93.4669, val_loss: 1.0550, val_accuracy: 75.2857
Epoch 47/50: train_loss: 0.1848, train_accuracy: 94.1822, val_loss: 1.1617, val_accuracy: 74.7143
Epoch 48/50: train_loss: 0.1970, train_accuracy: 93.4669, val_loss: 1.3389, val_accuracy: 75.0000
Epoch 49/50: train_loss: 0.2268, train_accuracy: 93.1330, val_loss: 1.2764, val_accuracy: 74.1429
Epoch 50/50: train_loss: 0.2165, train_accuracy: 93.4430, val_loss: 1.2749, val_accuracy: 74.0000

Test loss: 1.1350, test accuracy: 74.9763