{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yur1j27zVwwc",
        "colab_type": "text"
      },
      "source": [
        "# ATML report\n",
        "## Music genre recognition\n",
        "Authors:\n",
        "- Dorian Guyot\n",
        "- Jonathan Péclat\n",
        "- Thomas Schaller\n",
        "\n",
        "Goal : Classify music samples by genre\n",
        "## Dataset\n",
        "\n",
        "We used the GTZAN dataset (http://marsyas.info/downloads/datasets.html) in our work. It consists of ten genres, each one containing hundred samples of thirty seconds. The genres are: Blues, Classical, Country, Disco, Hiphop, Jazz, Metal, Pop, Reggae and Rock.\n",
        "\n",
        "## Approach description\n",
        "\n",
        "### Data preparation: spectrograms\n",
        "\n",
        "A music sample can be represented in multiple ways. Rather than direclty sending the raw bytes of the music file into a neural network, we chose to use a frequential domain representation produced through a FFT in the form of mel-spectrograms. We now have a 216x216 pixels monochromatic image describing our sample.\n",
        "\n",
        "On the generated image, the abscissa represents the time, and the ordinate the frequency. The brighter the pixel is, the stronger a frequency is played at a given time. The y axis is also a log-scale because humans hear music in a logarithmic fashion (going one octave higher doubles the frequency) and since music has been designed for human ears, it made sense to go down that road.\n",
        "\n",
        "**Remark**: It is worth noting that good results have also been reached with the raw data as inputs, but we will not use this approach.\n",
        "\n",
        "### Data augmentation\n",
        "\n",
        "The GTZAN dataset is pretty small and we are trying to train a (deep) neural network. We will therefore need to do some form of data augmentation to be able to reach interesting results and avoid overfitting.\n",
        "\n",
        "We tried and used different methods to increase the number of images from a given sample.\n",
        "  - Adding noise to the spectrograms to prevent the network to train on insignificant details and augment tolerance to low quality samples\n",
        "  - Splitting a sample into smaller samples without overlap to get more spectrograms\n",
        "  - Splitting a sample in a \"rolling window\" manner (into samples that do overlap)\n",
        "\n",
        "#### Further work\n",
        "Other ideas (mainly working on the raw audio) for more data augmentation have been considered but not implemented:\n",
        "  - Using additional representations for the samples (not only spectrograms)\n",
        "  - Pitch and tempo shifting\n",
        "  \n",
        "### Network architecture\n",
        "\n",
        "Having a representation of a song in the form of a picture opens the problem up to all the tools used in image processing and, amongst otherm makes it easy to use with a classic CNN. We tried out different types of network and got different results.\n",
        "\n",
        "#### The homebrew\n",
        "A first approach was obviously to try it ourselves and create a network architecture from scratch.\n",
        "\n",
        "The custom network has the following structure : \\\\\n",
        "Input -> 1x216x216 \\\\\n",
        "Conv2D -> 128x212x216 -> BatchNorm2D -> LeakyRelu 0.2 -> MaxPool2D -> 128x106x108 -> Dropout 0.5 \\\\\n",
        "Conv2D -> 64x102x108  -> BatchNorm2D -> LeakyRelu 0.2 -> MaxPool2D -> 64x51x54 -> Dropout 0.5 \\\\\n",
        "Conv2D -> 64x48x54 -> BatchNorm2D -> LeakyRelu 0.2 -> MaxPool2D -> 64x24x27 -> Dropout 0.5 \\\\\n",
        "Conv2D -> 64x24x27 -> BatchNorm2D -> LeakyRelu 0.2 -> MaxPool2D -> 64x10x27 Dropout 0.5 \\\\\n",
        "Linear -> 364 -> LeakyRelu -> Dropout 0.5 \\\\\n",
        "Linear -> 182 -> LeakyRelu -> Dropout 0.5 \\\\\n",
        "Linear -> 10 \\\\\n",
        "\n",
        "The results obtained with this architecture were quiet good (around 68%), given the difficult conditions (mainly the dataset size).\n",
        "\n",
        "#### Finetuning with ResNet\n",
        "Since the GTZAN dataset is really small, it was unthinkable to train a (very) deep network on it. The clear answer to this problem was to use some pre-trained network and transfer learning. We fine-tuned ResNet18 (pretrained on ImageNet) in this case but many other options such as VGG were considered. A linear classifier was appended to ResNet to fit the ten categories. Unsurprisingly, this yielded very good results (with an accuracy of over 95%)\n",
        "\n",
        "\n",
        "## Folder structure\n",
        "```\n",
        "ATML19\n",
        "│   README.md : Simple readme for github\n",
        "│   report.ipynb : Project report (this file)\n",
        "│   report.html : Same as report.ipynb but in HTML format\n",
        "│   pres.pdf : The pdf of the presentation\n",
        "│\n",
        "└─── notebooks : Notebook's files where tests were made\n",
        "│   │   create_data_folder.ipynb : Create data folder when spectrogram are created\n",
        "│   │   model_dorian.ipynb : Testing differents models on data\n",
        "│   │   model_thomas.ipynb : Testing differents models on data\n",
        "│   │   spectrogram.ipynb : Creating spectrograms from wav files\n",
        "│   │   generate_experiments.ipynb : Generate a barplot from genre classification on external wav's musics.\n",
        "│   │   src_python : Contains scripts use in generate_experiments (same as test_src)\n",
        "│   \n",
        "└─── test_src : Small app to test the final model\n",
        "│   │   user_app.py : Main file of the app\n",
        "│   └─── generate_data : Folder containing the file to create the spectogram of the music\n",
        "│   │    │ spectrogram.py : Class to create the spectrogram of the music\n",
        "│   │\n",
        "│   └─── models : Folder containing files for the model\n",
        "│   │    │ best_model_resnet : State dict of the best model created with resnet\n",
        "│   │    │ model.py : Model of the project. Allow to predict genre of music\n",
        "│   \n",
        "└─── train_src : Small app to test the final model\n",
        "│   │   main.py : Main file of the app to train the model\n",
        "│   └─── generate_data : Folder containing files to process the data and create the data directory.\n",
        "│   │    │ data_folder.py : Create the data folder to be able to use ImageFolder from pytorch then.\n",
        "│   │    │ spectrogram.py : Create the spectrograms of all musics contain in a folder.\n",
        "│   │\n",
        "│   └─── model : Folder containing files for training the model\n",
        "│   │    │ dataloader.py : Create the dataloader with the data of the data directory\n",
        "│   │    │ model.py : train the model with the dataloader and save the best one\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aybHDmezI_ze",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n",
        "There are 10 different genre of music to classify. Therefore an untrained network has a 1 in 10 chance of guessing right. 10% is thus the baseline.\n",
        "\n",
        "Both of the network have been trained for 50 epochs with ADAM, cross-entropy loss and an initial learning rate of 0.001 that decrease as the time goes by.\n",
        "\n",
        "## The homebrew\n",
        "With our custom architecture we have been able to reach an accuracy of about 68% (loss of 0.92) on the test set. Any more complex model tended to overfit and smaller models did never yield better results.\n",
        "\n",
        "## Fine-tuned ResNet\n",
        "As stated above the result with ResNet is on another level: we reached 95% accuracy (loss of 0.16) on the test set\n",
        "\n",
        "### Going further\n",
        "The custom network was only ever trained on the GTZAN dataset, which pretty much restricted the size of the network we could use. Pre-training the convolutionnal part of it on an unsupervised task involving spectrograms as inputs and then fine-tuning it on the exact task would be a great way to get the best out of both worlds (custom convolution kernels and transferred learning).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ci9aVtI8TN",
        "colab_type": "text"
      },
      "source": [
        "# Example of use\n",
        "\n",
        "## Training the network yourself\n",
        "If you want to train the network yourself you will need some additional data. You can either download the raw wav files and do the processing by yourself with our app, or directly download the already processed data.\n",
        "\n",
        "The `data` folder contains the spectrograms of the wav files. Here is a link to download the two folders: https://www.dropbox.com/sh/dg1crj9yimefgpb/AADcOLk9fkLxFbaO7dn-rACDa?dl=0\n",
        "\n",
        "## Classifying music with the app\n",
        "If you want to use the trained network with no hassle to predict the genre of a song, you can simply use the small app we coded for that pusrpose. Just run the python file in the folder \"test_src\" named \"user_app.py\". You then have a very basic graphical user interface that enable you to select a file with a button on the top of the window, and them hit the button \"classify\". The output is the probability the network gives to each genre for the song you gave as input.\n",
        "\n",
        "### How does the app classify an entire song ?\n",
        "The app classifies a whole song into a category. Since the network only ever works on small slices of a few seconds, the app has to do some additional work. What it actually does is similar to the data preparation / data augmentation, namely it runs a sliding window over the wole song, classifies all the slices and then finally sums up all the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C5ZerMANXMC",
        "colab_type": "text"
      },
      "source": [
        "## Classifying the music programmatically\n",
        "If you don't want to use the app or want to classify the music in your own way, you can do so by using our code as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0JNfoqNNYZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from test_src.generate_data.spectrogram import Spectrogram\n",
        "from test_src.models.model import Model\n",
        "\n",
        "data_folder = 'test_data'\n",
        "styles = ['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']\n",
        "\n",
        "model_dict_path = \"test_src/models/best_model_resnet\"\n",
        "model = Model()\n",
        "model.load(model_dict_path)\n",
        "spectrogram = Spectrogram()\n",
        "length_data = len(test_data)\n",
        "\n",
        "imgs = spectrogram.sample(os.path.join(experiments_folder,data[0],data[1]))\n",
        "results_sum = np.array([0.0] * len(styles))\n",
        "for img in imgs:\n",
        "    results_sum += model.predict_image(img)\n",
        "results = results_sum / len(imgs) * 100.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuu231wZWqFi",
        "colab_type": "text"
      },
      "source": [
        "## Generating experiments\n",
        "\n",
        "We have also run our network on full-length and well-known songs that were absolutely not in the dataset. Here are the code snippets to make this and help to understand how everything works together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skE2jJj5MGDZ",
        "colab_type": "text"
      },
      "source": [
        "**Beware**\n",
        "\n",
        "The code below can also be found in the notebook `generate_experiments`.\n",
        "\n",
        "The folder that contains the music to be processed below is located at the root of the project under the name `test_data`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy_cBmpPMCCS",
        "colab_type": "text"
      },
      "source": [
        "To begin, we have to make the necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLuC86vpWraB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from test_src.generate_data.spectrogram import Spectrogram\n",
        "from test_src.models.model import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J951YGs9GxTW",
        "colab_type": "text"
      },
      "source": [
        "Setup matplotlib to work in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYLh3LkvGvqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3n29n2lazjS",
        "colab_type": "text"
      },
      "source": [
        "Then, list the music in the designated folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j-DJdzoa1BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "experiments_folder = 'test_data'\n",
        "styles = ['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM5fOMqra1pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = []\n",
        "for file in os.listdir(experiments_folder):\n",
        "    if os.path.isdir(os.path.join(experiments_folder, file)):\n",
        "        for file2 in os.listdir(os.path.join(experiments_folder, file)):\n",
        "            if os.path.isfile(os.path.join(experiments_folder,file,file2)):\n",
        "                test_data.append([file, file2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXVT0xdMa3zD",
        "colab_type": "text"
      },
      "source": [
        "Now, we need to instantiate a class that will help us with the generation of the spectrograms aswell as the classifier (whose weights are stored at `test_src/models/best_model_resnet`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tHKcdewa5DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dict_path = \"test_src/models/best_model_resnet\"\n",
        "model = Model()\n",
        "model.load(model_dict_path)\n",
        "spectrogram = Spectrogram()\n",
        "length_data = len(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK8khyzQa6W9",
        "colab_type": "text"
      },
      "source": [
        "Finally, for each song we generate its corresponding spectrograms, and predict the genre using out model. Then, to better visualize the result, a BarPlot is created with the percentage obtained for each of the genres. (Remember that we classify each slice and the sum them up)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-2Vzw-a7hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "for data in test_data:\n",
        "    print(\"Processing music \",i,\"/\",length_data)\n",
        "    imgs = spectrogram.sample(os.path.join(experiments_folder,data[0],data[1]))\n",
        "    results_sum = np.array([0.0] * len(styles))\n",
        "    for img in imgs:\n",
        "        results_sum += model.predict_image(img)\n",
        "    results = results_sum / len(imgs) * 100.0\n",
        "    y_pos = np.arange(len(styles))\n",
        "    plt.bar(y_pos, results, align='center', alpha=0.5)\n",
        "    plt.xticks(y_pos, styles, rotation=45)\n",
        "    plt.ylim([0,100])\n",
        "    plt.ylabel('Percent')\n",
        "    plt.title('Title: '+data[1]+', True genre: '+data[0])\n",
        "    plt.show()\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}