{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yur1j27zVwwc"
   },
   "source": [
    "# ATML report\n",
    "## Music genre recognition\n",
    "Authors:\n",
    "- Dorian Guyot\n",
    "- Jonathan Péclat\n",
    "- Thomas Schaller\n",
    "\n",
    "Goal : Classify music samples by genre\n",
    "## Dataset\n",
    "\n",
    "We used the GTZAN dataset (http://marsyas.info/downloads/datasets.html) in our work. It consists of ten genres, each one containing hundred samples of thirty seconds. The genres are: Blues, Classical, Country, Disco, Hiphop, Jazz, Metal, Pop, Reggae and Rock.\n",
    "\n",
    "## Approach description\n",
    "\n",
    "### Data preparation: spectrograms\n",
    "\n",
    "A music sample can be represented in multiple ways. Rather than direclty sending the raw bytes of the music file into a neural network, we chose to use a frequential domain representation produced through a FFT in the form of spectrograms. We now have a 216x216 pixels monochromatic image describing our sample.\n",
    "\n",
    "On the generated image, the abscissa represents the time, and the ordinate the frequency. The brighter the pixel is, the stronger a frequency is played at a given time. The y axis is also a log-scale because humans hear music in a logarithmic fashion (going one octave higher doubles the frequency) and since music has been designed for human ears, it made sense to go down that road.\n",
    "\n",
    "**Remark**: It is worth noting that good results have also been reached with the raw data as inputs, but we will not use this approach.\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "The GTZAN dataset is pretty small and we are trying to train a (deep) neural network. We will therefore need to do some form of data augmentation to be able to reach interesting results and avoid overfitting.\n",
    "\n",
    "We tried and used different methods to increase the number of images from a given sample.\n",
    "  - Adding noise to the spectrograms to prevent the network to train on insignificant details and augment tolerance to low quality samples. This was a very significant element and increased the accuracy of about 5%.\n",
    "  - Splitting a sample into smaller samples without overlap to get more spectrograms\n",
    "  - Splitting a sample in a \"rolling window\" manner (into samples that do overlap)\n",
    "\n",
    "#### Further work\n",
    "Other ideas (mainly working on the raw audio) for more data augmentation have been considered but not implemented:\n",
    "  - Using additional representations for the samples (not only spectrograms)\n",
    "  - Pitch and tempo shifting\n",
    "  \n",
    "### Network architecture\n",
    "\n",
    "Having a representation of a song in the form of a picture opens the problem up to all the tools used in image processing and, amongst other makes it easy to use with a classic CNN. We tried out different types of network and got different results.\n",
    "\n",
    "#### The homebrew\n",
    "A first approach was obviously to try it ourselves and create a network architecture from scratch.\n",
    "\n",
    "The custom network has the following structure : \\\\\n",
    "\n",
    "Input -> 1x216x216 \\\\\n",
    "\n",
    "Conv2D -> 128x212x216 -> BatchNorm2D -> LeakyRelu 0.2 -> Max\n",
    "\n",
    "Pool2D -> 128x106x108 -> Dropout 0.5 \\\\\n",
    "\n",
    "Conv2D -> 64x102x108  -> BatchNorm2D -> LeakyRelu 0.2 -> MaxPool2D -> 64x51x54 -> Dropout 0.5 \\\\\n",
    "\n",
    "Conv2D -> 64x48x54 -> BatchNorm2D -> LeakyRelu 0.2 -> MaxPool2D -> 64x24x27 -> Dropout 0.5 \\\\\n",
    "\n",
    "Conv2D -> 64x24x27 -> BatchNorm2D -> LeakyRelu 0.2 -> MaxPool2D -> 64x10x27 Dropout 0.5 \\\\\n",
    "\n",
    "Linear -> 364 -> LeakyRelu -> Dropout 0.5 \\\\\n",
    "\n",
    "Linear -> 182 -> LeakyRelu -> Dropout 0.5 \\\\\n",
    "\n",
    "Linear -> 10 \\\\\n",
    "\n",
    "The results obtained with this architecture were quiet good (around 75%), given the difficult conditions (mainly the dataset size).\n",
    "\n",
    "#### Finetuning with ResNet\n",
    "Since the GTZAN dataset is really small, it was unthinkable to train a (very) deep network on it. The clear answer to this problem was to use some pre-trained network and transfer learning. We fine-tuned ResNet18 (pretrained on ImageNet) in this case but many other options such as VGG were considered. A linear classifier was appended to ResNet to fit the ten categories. Unsurprisingly, this yielded very good results (with an accuracy of over 95%)\n",
    "\n",
    "\n",
    "## Folder structure\n",
    "```\n",
    "ATML19\n",
    "│   README.md : Simple readme for github\n",
    "│   report.ipynb : Project report (this file)\n",
    "│   report.html : Same as report.ipynb but in HTML format\n",
    "│   pres.pdf : The pdf of the presentation\n",
    "│\n",
    "└─── notebooks : Notebook's files where tests were made\n",
    "│   │   create_data_folder.ipynb : Create data folder when spectrogram are created\n",
    "│   │   model_dorian.ipynb : Testing differents models on data\n",
    "│   │   model_thomas.ipynb : Testing differents models on data\n",
    "│   │   spectrogram.ipynb : Creating spectrograms from wav files\n",
    "│   │   generate_experiments.ipynb : Generate a barplot from genre classification on external wav's musics.\n",
    "│   │   src_python : Contains scripts use in generate_experiments (same as test_src)\n",
    "│   \n",
    "└─── test_src : Small app to test the final model\n",
    "│   │   user_app.py : Main file of the app\n",
    "│   └─── generate_data : Folder containing the file to create the spectogram of the music\n",
    "│   │    │ spectrogram.py : Class to create the spectrogram of the music\n",
    "│   │\n",
    "│   └─── models : Folder containing files for the model\n",
    "│   │    │ best_model_resnet : State dict of the best model created with resnet\n",
    "│   │    │ model.py : Model of the project. Allow to predict genre of music\n",
    "│   \n",
    "└─── train_src : Small app to test the final model\n",
    "│   │   main.py : Main file of the app to train the model\n",
    "│   └─── generate_data : Folder containing files to process the data and create the data directory.\n",
    "│   │    │ data_folder.py : Create the data folder to be able to use ImageFolder from pytorch then.\n",
    "│   │    │ spectrogram.py : Create the spectrograms of all musics contain in a folder.\n",
    "│   │\n",
    "│   └─── model : Folder containing files for training the model\n",
    "│   │    │ dataloader.py : Create the dataloader with the data of the data directory\n",
    "│   │    │ model.py : train the model with the dataloader and save the best one\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aybHDmezI_ze"
   },
   "source": [
    "# Results\n",
    "\n",
    "There are 10 different genre of music to classify. Therefore an untrained network has a 1 in 10 chance of guessing right. 10% is thus the baseline.\n",
    "\n",
    "Both of the network have been trained for 50 epochs with ADAM, cross-entropy loss and an initial learning rate of 0.001 that decrease as the time goes by.\n",
    "\n",
    "## The homebrew\n",
    "We started with a very simple model and increased the complexity gradually until we reached a sweetspot just bedore overfitting. The best results we have been able to reach are an accuracy of about 75% (loss of 1.13) on the test set. \n",
    "\n",
    "<img src=\"http://dragoo.ch/images/ATML/best_homebrew.png\" width = 80%>\n",
    "\n",
    "Any more complex model tended to overfit. The graph below is from the same network as the above, but with one more convolutional layer. (The training has been interupted because the overfitting was clear)\n",
    "\n",
    "<img src=\"http://dragoo.ch/images/ATML/homebrew_overfit.png\" width = 80%>\n",
    "\n",
    "\n",
    "## Transfer learning with ResNet\n",
    "\n",
    "Since the dataset is small and we are working on images, it made sense to consider transfer learning with one of the well-known image classification networks such as VGG, ResNet or AlexNet.\n",
    "\n",
    "We used a ResNet pretrained on ImageNet. While it is true that spectrograms are not at all like objects from the ImageNet dataset, the low-level features may come in handy for the \"understanding\" that the networks builds from the former, especially when fine-tuning.\n",
    "\n",
    "\n",
    "### Fixed features\n",
    "With fixed features, we were able to reach an accuracy of about 68% (loss of 0.92), which is not bad considering the fact that spectrograms are not at all present in ImageNet, on which ResNet had been trained.\n",
    "<img src=\"http://dragoo.ch/images/ATML/resnet_fixed_feature.png\" width = 80%>\n",
    "\n",
    "### Finetuning\n",
    "\n",
    "Using a pretrained ResNet and fine-tuning it to adapt to spectrograms has led to the best results we were able to achieve with 95% accuracy (loss of 0.16) on the test set.\n",
    "\n",
    "<img src=\"http://dragoo.ch/images/ATML/rewsnet_fine_tuning.png\" width = 80%>\n",
    "\n",
    "### Conclusion\n",
    "As could be expected in our setting (small dataset) the best option was to use transfer learning and fine-tuning. We can see in both graphs using ResNet that the network learns really fast and then stagnates. The fast convergence is probably due to the depth of the network and the plateau could be explained by the small size of the dataset: the network is able to learn its task really fast due to the complexity it has at hand, exhausts all the information present in the dataset and is then not able to learn further anymore.\n",
    "\n",
    "### Going further\n",
    "The custom network was only ever trained on the GTZAN dataset, which pretty much restricted the size of the network we could making use of transfer learning. Pre-training the convolutionnal part of it on an unsupervised task involving spectrograms as inputs and then fine-tuning it on the exact task would be a great way to get the best out of both worlds (custom convolution kernels and transferred learning).\n",
    "\n",
    "It would also be really beneficial to work on a bigger dataset. It would open the door of complexity to our own model and provide more information for the ResNet models to learn further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObBeRJUlyKS0"
   },
   "source": [
    "# Experiments\n",
    "\n",
    "We have tested the classification on songs outside the dataset and have gotten different results:\n",
    "\n",
    "**Very good results**\n",
    "\n",
    "A significant proportion of the tested songs were classified correctly and some of them with great confidence. This was mostly the case for music genres that were also really distinguishable from the others, such as classical or reggae.\n",
    "\n",
    "<img src=\"http://dragoo.ch/images/ATML/good_classification.PNG\" img>\n",
    "\n",
    "**Understandable bad results**\n",
    "\n",
    "Since even for humans the classification is not always clear, we cannot expect the network to do a lot better. We had several cases of wrong classification that were \"understandable\" in the sense that the song was close to another genre, or a mix of both. In this case, one can argue that the rock, jazz and blues genres sometimes overlap.\n",
    "\n",
    "<img src=\"http://dragoo.ch/images/ATML/understandable_classification.PNG\" img>\n",
    "\n",
    "**Bad results**\n",
    "\n",
    "Of course we also had results that were just completely wrongly classified with no appearing similarities. The network is not very confident that it is right and the right genre has low probability. An example can be seen below (the true genre is metal, but it gets classified as pop and the metal probability is nearly zero):\n",
    "\n",
    "<img src=\"http://dragoo.ch/images/ATML/bad_classification.PNG\" img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e86IOjiP5cHc"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Music genre recognition is clearly feasible but should be interpreted in a fuzzy manner: a song can belong to multiples genres depending on the instruments present in the piece, the rythm or even the lyrics. A single music genre covers a lot of sub-genres and it is virtually impossible to classify everthing correctly since even humans argue about this. It would be great to have a bigger dataset with multiple genres assigned to each song to enable a better training and a more meaningful classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30ci9aVtI8TN"
   },
   "source": [
    "# Example of use\n",
    "\n",
    "## Training the network yourself\n",
    "If you want to train the network yourself you will need some additional data. You can either download the raw wav files and do the processing by yourself with our app, or directly download the already processed data.\n",
    "\n",
    "The `data` folder contains the spectrograms of the wav files. Here is a link to download the two folders: https://www.dropbox.com/sh/dg1crj9yimefgpb/AADcOLk9fkLxFbaO7dn-rACDa?dl=0\n",
    "\n",
    "## Classifying music with the app\n",
    "If you want to use the trained network with no hassle to predict the genre of a song, you can simply use the small app we coded for that pusrpose. Just run the python file in the folder \"test_src\" named \"user_app.py\". You then have a very basic graphical user interface that enable you to select a file with a button on the top of the window, and them hit the button \"classify\". The output is the probability the network gives to each genre for the song you gave as input.\n",
    "\n",
    "### How does the app classify an entire song ?\n",
    "The app classifies a whole song into a category. Since the network only ever works on small slices of a few seconds, the app has to do some additional work. What it actually does is similar to the data preparation / data augmentation, namely it runs a sliding window over the wole song, classifies all the slices and then finally sums up all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1C5ZerMANXMC"
   },
   "source": [
    "## Classifying the music programmatically\n",
    "If you don't want to use the app or want to classify the music in your own way, you can do so by using our code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0JNfoqNNYZs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from test_src.generate_data.spectrogram import Spectrogram\n",
    "from test_src.models.model import Model\n",
    "\n",
    "data_folder = 'test_data'\n",
    "styles = ['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']\n",
    "\n",
    "# Create the model\n",
    "model_dict_path = \"test_src/models/best_model_resnet\"\n",
    "model = Model()\n",
    "model.load(model_dict_path)\n",
    "\n",
    "# Generate all spectrograms\n",
    "spectrogram = Spectrogram()\n",
    "imgs = spectrogram.sample( path/to/the/song )\n",
    "\n",
    "# Predict the style\n",
    "results_sum = np.array([0.0] * len(styles))\n",
    "for img in imgs:\n",
    "    results_sum += model.predict_image(img)\n",
    "results = results_sum / len(imgs) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuu231wZWqFi"
   },
   "source": [
    "## Generating experiments\n",
    "\n",
    "We have also run our network on full-length and well-known songs that were absolutely not in the dataset. Here are the code snippets to make this and help to understand how everything works together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skE2jJj5MGDZ"
   },
   "source": [
    "**Remark**\n",
    "\n",
    "The code below can also be found in the notebook `generate_experiments`.\n",
    "\n",
    "The folder that contains the music to be processed below is located at the root of the project under the name `test_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cy_cBmpPMCCS"
   },
   "source": [
    "To begin, we have to make the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLuC86vpWraB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from test_src.generate_data.spectrogram import Spectrogram\n",
    "from test_src.models.model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J951YGs9GxTW"
   },
   "source": [
    "Setup matplotlib to work in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYLh3LkvGvqg"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3n29n2lazjS"
   },
   "source": [
    "Then, list the music in the designated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9j-DJdzoa1BN"
   },
   "outputs": [],
   "source": [
    "experiments_folder = 'test_data'\n",
    "styles = ['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM5fOMqra1pn"
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for file in os.listdir(experiments_folder):\n",
    "    if os.path.isdir(os.path.join(experiments_folder, file)):\n",
    "        for file2 in os.listdir(os.path.join(experiments_folder, file)):\n",
    "            if os.path.isfile(os.path.join(experiments_folder,file,file2)):\n",
    "                test_data.append([file, file2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXVT0xdMa3zD"
   },
   "source": [
    "Now, we need to instantiate a class that will help us with the generation of the spectrograms aswell as the classifier (whose weights are stored at `test_src/models/best_model_resnet`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0tHKcdewa5DN"
   },
   "outputs": [],
   "source": [
    "model_dict_path = \"test_src/models/best_model_resnet\"\n",
    "model = Model()\n",
    "model.load(model_dict_path)\n",
    "spectrogram = Spectrogram()\n",
    "length_data = len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DK8khyzQa6W9"
   },
   "source": [
    "Finally, for each song we generate its corresponding spectrograms, and predict the genre using out model. Then, to better visualize the result, a BarPlot is created with the percentage obtained for each of the genres. (Remember that we classify each slice and the sum them up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jK-2Vzw-a7hu"
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "for data in test_data:\n",
    "    print(\"Processing music \",i,\"/\",length_data)\n",
    "    imgs = spectrogram.sample(os.path.join(experiments_folder,data[0],data[1]))\n",
    "    results_sum = np.array([0.0] * len(styles))\n",
    "    for img in imgs:\n",
    "        results_sum += model.predict_image(img)\n",
    "    results = results_sum / len(imgs) * 100.0\n",
    "    y_pos = np.arange(len(styles))\n",
    "    plt.bar(y_pos, results, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, styles, rotation=45)\n",
    "    plt.ylim([0,100])\n",
    "    plt.ylabel('Percent')\n",
    "    plt.title('Title: '+data[1]+', True genre: '+data[0])\n",
    "    plt.show()\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "report.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
